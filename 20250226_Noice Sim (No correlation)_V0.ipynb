{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Study for Noise Filtering\n",
    "\n",
    "This is the v0 for the simulation study on the sparse jump model comparison with HMM, to show that SJM is able to filter away noisy data by using the weighting in the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from jumpmodels.sparse_jump import SparseJumpModel    # Sparse JM class\n",
    "from jumpmodels.jump import JumpModel    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Simulation & Utility Functions\n",
    "def simulate_data(T, P, mu, random_state=None): \"\"\" Simulate data from a 3-state Gaussian HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data(T, P, mu, random_state=None):\n",
    "    \"\"\"\n",
    "    Simulate data from a 3-state Gaussian HMM.\n",
    "    \n",
    "    Parameters:\n",
    "        T (int): Number of observations.\n",
    "        P (int): Total number of features (only first 15 are informative).\n",
    "        mu (float): Signal magnitude for informative features.\n",
    "        random_state (int or None): Seed for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "        X (ndarray): Simulated observations (T x P).\n",
    "        states (ndarray): True state sequence (length T).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    \n",
    "    # Transition matrix as given in your original code\n",
    "    transmat = np.array([[0.9903, 0.0047, 0.0050],\n",
    "                         [0.0157, 0.9666, 0.0177],\n",
    "                         [0.0284, 0.0300, 0.9416]])\n",
    "    transmat = transmat / transmat.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Compute stationary distribution (eigenvector corresponding to eigenvalue 1)\n",
    "    eigvals, eigvecs = np.linalg.eig(transmat.T)\n",
    "    stat = np.real(eigvecs[:, np.isclose(eigvals, 1)])\n",
    "    stat = stat[:, 0]\n",
    "    stat = stat / np.sum(stat)\n",
    "    \n",
    "    # Generate state sequence\n",
    "    states = np.zeros(T, dtype=int)\n",
    "    states[0] = rng.choice(np.arange(3), p=stat)\n",
    "    for t in range(1, T):\n",
    "        states[t] = rng.choice(np.arange(3), p=transmat[states[t-1]])\n",
    "    \n",
    "    # Define means for each state\n",
    "    means = np.zeros((3, P))\n",
    "    # State 0: +mu in first 15 features\n",
    "    # State 1: 0\n",
    "    # State 2: -mu in first 15 features\n",
    "    if P >= 15:\n",
    "        means[0, :15] = mu\n",
    "        means[2, :15] = -mu\n",
    "    else:\n",
    "        means[0, :P] = mu\n",
    "        means[2, :P] = -mu\n",
    "    \n",
    "    # Generate observations: N(means[state], I_P)\n",
    "    X = np.zeros((T, P))\n",
    "    for t in range(T):\n",
    "        X[t] = rng.normal(loc=means[states[t]], scale=1.0, size=P)\n",
    "    \n",
    "    return X, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Aligning Predicted Labels With True Labels using the Hungarian Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_labels(true_labels, pred_labels):\n",
    "    \"\"\"\n",
    "    Align predicted labels with true labels using the Hungarian algorithm.\n",
    "    \n",
    "    Returns:\n",
    "        aligned (ndarray): Predicted labels after optimal permutation.\n",
    "    \"\"\"\n",
    "    D = confusion_matrix(true_labels, pred_labels)\n",
    "    row_ind, col_ind = linear_sum_assignment(-D)\n",
    "    mapping = {col: row for row, col in zip(row_ind, col_ind)}\n",
    "    aligned = np.array([mapping[x] for x in pred_labels])\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up the function to calcuate the BAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bac(true_states, pred_states):\n",
    "    \"\"\"\n",
    "    Compute the Balanced Accuracy (BAC) after aligning the predicted state labels.\n",
    "    \"\"\"\n",
    "    aligned_pred = align_labels(true_states, pred_states)\n",
    "    return balanced_accuracy_score(true_states, aligned_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions for model formulation\n",
    "\n",
    "### 4.1 HMM With Nystrup (2021) initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hmm(X, n_components=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Fit a Gaussian HMM to the data X with the following initialization:\n",
    "      - Self-transition probability set to 0.95.\n",
    "      - Covariance prior set to 1.0 (for regularization).\n",
    "      - Up to 100 iterations of the EM algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Data matrix.\n",
    "        n_components (int): Number of hidden states.\n",
    "        random_state (int or None): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        pred_states (ndarray): Predicted state sequence using Viterbi decoding.\n",
    "    \"\"\"\n",
    "    model = hmm.GaussianHMM(\n",
    "        n_components=n_components,             # Number of hidden states\n",
    "        covariance_type='diag',                # Diagonal covariance matrices\n",
    "        n_iter=100,                            # Maximum number of EM iterations\n",
    "        random_state=random_state,             # Seed for reproducibility\n",
    "        init_params=\"mc\",                      # Initialize means ('m') and covariances ('c')\n",
    "        covars_prior=1.0                   # Regularization: prior added to covariance estimates\n",
    "    )\n",
    "    # Set uniform start probabilities\n",
    "    model.startprob_ = np.full(n_components, 1.0 / n_components)\n",
    "    # Initialize transition matrix: 0.95 on the diagonal, the remaining probability spread evenly\n",
    "    transmat = np.full((n_components, n_components), 0.05 / (n_components - 1))\n",
    "    np.fill_diagonal(transmat, 0.95)\n",
    "    model.transmat_ = transmat\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    model.fit(X)\n",
    "    # Predict the hidden state sequence using the Viterbi algorithm\n",
    "    pred_states = model.predict(X)\n",
    "    return pred_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Normal (Standard) Jump Model with Grid Search over Î»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_jump_model_grid_search(X, true_states, n_components=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Perform a grid search over 14 lambda values (logspace from 1e-2 to 1e4) for the jump model.\n",
    "    \n",
    "    For each lambda value:\n",
    "      - A JumpModel is initialized and fitted.\n",
    "      - The jump penalty (lambda) controls the cost of switching states.\n",
    "        - A low lambda allows frequent state changes.\n",
    "        - A high lambda penalizes state changes, resulting in fewer jumps.\n",
    "      - The parameter 'cont' specifies whether the jump model is continuous (True) or discrete (False).\n",
    "      - 'max_iter' defines the maximum number of iterations for the model fitting procedure.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Data matrix.\n",
    "        true_states (ndarray): The true hidden state sequence.\n",
    "        n_components (int): Number of states.\n",
    "        random_state (int or None): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        best_labels (ndarray): Predicted state sequence for the best lambda.\n",
    "        best_bac (float): Best balanced accuracy achieved.\n",
    "    \"\"\"\n",
    "    # Create 14 lambda values logarithmically spaced from 0.001 to 10,000.\n",
    "    lambda_values = np.logspace(-2, 4, 14)\n",
    "    best_bac = -1\n",
    "    best_labels = None\n",
    "    \n",
    "    for lam in lambda_values:\n",
    "        # Create a JumpModel instance with the following parameters:\n",
    "        model = JumpModel(\n",
    "            n_components=n_components,    # Number of hidden states\n",
    "            jump_penalty=lam,             # Lambda: penalty for a state transition\n",
    "            cont=False,                   # 'cont': if False, model uses discrete jumps\n",
    "            max_iter=10,                  # Maximum number of iterations for fitting the model\n",
    "            random_state=random_state     # Seed for reproducibility\n",
    "        )\n",
    "        # Fit the jump model to the data X\n",
    "        model.fit(X)\n",
    "        # Retrieve predicted state labels from the model\n",
    "        labels = model.labels_\n",
    "        # Calculate Balanced Accuracy (BAC) after aligning predicted labels with true states\n",
    "        bac = calculate_bac(true_states, labels)\n",
    "        # Update the best result if this lambda gives a higher BAC\n",
    "        if bac > best_bac:\n",
    "            best_bac = bac\n",
    "            best_labels = labels\n",
    "    \n",
    "    return best_labels, best_bac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Sparse Jump Model with Grid Search over Î» and kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sparse_jump_model_grid_search(X, true_states, n_components=3, random_state=None):\n",
    "    \"\"\"\n",
    "    Perform a grid search for the best combination of jump_penalty (lambda) and feature selection\n",
    "    level for the Sparse Jump Model (SJM). In SJM, feature selection is controlled by 'max_feats',\n",
    "    which is defined as the square of 'kappa'. Here we vary kappa from 1 to sqrt(P) and set\n",
    "    max_feats = kappa**2.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        Data matrix of shape (T, P) where T is the number of observations and P the number of features.\n",
    "    true_states : ndarray\n",
    "        The true hidden state sequence.\n",
    "    n_components : int, default=3\n",
    "        Number of hidden states (clusters).\n",
    "    random_state : int or None, optional\n",
    "        Seed for reproducibility.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_labels : ndarray\n",
    "        Predicted state sequence for the best combination of parameters.\n",
    "    best_bac : float\n",
    "        The best balanced accuracy achieved.\n",
    "    \n",
    "    Model Parameters (as per documentation)\n",
    "    -----------------------------------------\n",
    "    - jump_penalty : float\n",
    "        The penalty for state transitions. In the SJM, this penalty is internally scaled by 1/sqrt(n_features).\n",
    "    - cont : bool, default=False\n",
    "        Use discrete jumps (False) rather than continuous.\n",
    "    - max_feats : float, default=100.\n",
    "        Controls the number of features included. This is set to kappa^2.\n",
    "    - max_iter : int, default=30\n",
    "        Maximum number of iterations for the coordinate descent algorithm (feature selection).\n",
    "    - Other parameters (tol_w, max_iter_jm, tol_jm, n_init_jm, verbose) use their default values.\n",
    "    \"\"\"\n",
    "    # Define 7 lambda values on a log-scale from 10^-1 to 10^2.\n",
    "    lambdas = np.logspace(-1, 2, 7)\n",
    "    p = X.shape[1]  # Total number of features\n",
    "    # Define 14 kappa values ranging from 1 to sqrt(P).\n",
    "    kappas = np.linspace(1, np.sqrt(p), 14)\n",
    "    \n",
    "    best_bac = -1\n",
    "    best_labels = None\n",
    "    \n",
    "    # Grid search over all combinations of lambda and kappa.\n",
    "    for lam in lambdas:\n",
    "        for kappa in kappas:\n",
    "            # Compute max_feats as the square of kappa (as per documentation).\n",
    "            max_feats = kappa**2\n",
    "            \n",
    "            # Create the SparseJumpModel instance.\n",
    "            # Key parameters:\n",
    "            # - n_components: number of states.\n",
    "            # - jump_penalty: lambda value controlling cost of switching states.\n",
    "            # - cont: set to False for the discrete jump model.\n",
    "            # - max_feats: effective number of features (kappa^2).\n",
    "            # - max_iter: maximum iterations for the coordinate descent algorithm (default=30 per docs).\n",
    "            # - random_state: seed for reproducibility.\n",
    "            model = SparseJumpModel(\n",
    "                n_components=n_components,\n",
    "                jump_penalty=lam,\n",
    "                cont=False,\n",
    "                max_feats=max_feats,     # effective number of features = kappa^2\n",
    "                max_iter=30,             # default from documentation (30 iterations)\n",
    "                random_state=random_state\n",
    "                # Additional parameters such as tol_w, max_iter_jm, tol_jm, n_init_jm, and verbose\n",
    "                # will use their default values.\n",
    "            )\n",
    "            # Fit the Sparse Jump Model to the data.\n",
    "            model.fit(X)\n",
    "            # Retrieve the predicted state labels.\n",
    "            labels = model.labels_\n",
    "            # Calculate the Balanced Accuracy (BAC) by aligning labels to true states.\n",
    "            bac = calculate_bac(true_states, labels)\n",
    "            # Update best_bac and best_labels if this combination yields a higher BAC.\n",
    "            if bac > best_bac:\n",
    "                best_bac = bac\n",
    "                best_labels = labels\n",
    "    \n",
    "    return best_labels, best_bac\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Main Execution\n",
    " We split the code into three sections for each model and then combine results at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Settings ---\n",
    "    # T: Number of time points/observations.\n",
    "    # mu_values: List of signal magnitudes for the informative features.\n",
    "    # p_values: List of numbers of features (dimensions) to simulate.\n",
    "    # n_simulations: Number of simulation runs per (mu, P) combination.\n",
    "    T = 500\n",
    "    mu_values = [0.25, 0.5, 0.75, 1.0]   # Different signal magnitudes\n",
    "    p_values = [15, 30, 60, 150, 300]      # Different numbers of features\n",
    "    n_simulations = 10                   # More simulations yield more robust results\n",
    "    \n",
    "    # final_rows will store the summary results for each (mu, P) combination.\n",
    "    final_rows = []\n",
    "    \n",
    "    # Loop over each combination of signal magnitude and number of features.\n",
    "    for mu in mu_values:\n",
    "        for P in p_values:\n",
    "            \n",
    "            # Create lists to collect Balanced Accuracy (BAC) scores from each model\n",
    "            # across multiple simulation runs.\n",
    "            hmm_bac_list = []      # For the Gaussian HMM\n",
    "            jump_bac_list = []     # For the standard Jump Model\n",
    "            sparse_bac_list = []   # For the Sparse Jump Model\n",
    "            \n",
    "            # Run multiple simulations to obtain robust performance estimates.\n",
    "            for sim in range(n_simulations):\n",
    "                # Simulate data for the current settings:\n",
    "                # X: data matrix of shape (T, P)\n",
    "                # true_states: true underlying state sequence.\n",
    "                X, true_states = simulate_data(T, P, mu, random_state=sim)\n",
    "                \n",
    "                # ----------------- 1) Gaussian HMM -------------------\n",
    "                # Fit the Gaussian HMM (with Nystrup 2021 initialization) and predict states.\n",
    "                pred_hmm = run_hmm(X, n_components=3, random_state=sim)\n",
    "                # Calculate the Balanced Accuracy (BAC) by aligning predicted states with true states.\n",
    "                bac_hmm = calculate_bac(true_states, pred_hmm)\n",
    "                # Save the BAC score.\n",
    "                hmm_bac_list.append(bac_hmm)\n",
    "                \n",
    "                # ----------------- 2) Normal Jump Model --------------\n",
    "                # Perform grid search over lambda values for the standard Jump Model.\n",
    "                # jump_penalty (lambda) controls the cost of switching states.\n",
    "                # 'cont=False' specifies that the model uses discrete state jumps.\n",
    "                # 'max_iter=10' limits the number of iterations for model convergence.\n",
    "                _, bac_jump = run_jump_model_grid_search(X, true_states, \n",
    "                                                         n_components=3, \n",
    "                                                         random_state=sim)\n",
    "                # Save the BAC score.\n",
    "                jump_bac_list.append(bac_jump)\n",
    "                \n",
    "                # ----------------- 3) Sparse Jump Model --------------\n",
    "                # Perform grid search over a combination of lambda and kappa values.\n",
    "                # lambda (jump_penalty) controls the cost of state transitions.\n",
    "                # kappa controls the sparsity (feature selection) in the model.\n",
    "                # 'max_feats' is computed as kappa**2.\n",
    "                # 'max_iter=30' is used per documentation.\n",
    "                _, bac_sparse = run_sparse_jump_model_grid_search(X, true_states,\n",
    "                                                                  n_components=3,\n",
    "                                                                  random_state=sim)\n",
    "                # Save the BAC score.\n",
    "                sparse_bac_list.append(bac_sparse)\n",
    "            \n",
    "            # Compute the mean and standard deviation of BAC scores for each method.\n",
    "            hmm_mean, hmm_std = np.mean(hmm_bac_list), np.std(hmm_bac_list)\n",
    "            jump_mean, jump_std = np.mean(jump_bac_list), np.std(jump_bac_list)\n",
    "            sparse_mean, sparse_std = np.mean(sparse_bac_list), np.std(sparse_bac_list)\n",
    "            \n",
    "            # Perform a paired t-test between the standard Jump Model and the Sparse Jump Model.\n",
    "            # The null hypothesis is that their mean BAC scores are equal.\n",
    "            tstat, pval = stats.ttest_rel(jump_bac_list, sparse_bac_list)\n",
    "            # If the Sparse Jump Model has a significantly higher mean (p < 0.05), mark it in bold.\n",
    "            if (pval < 0.05) and (sparse_mean > jump_mean):\n",
    "                sparse_str = f\"**{sparse_mean:.2f} Â± {sparse_std:.2f}**\"\n",
    "            else:\n",
    "                sparse_str = f\"{sparse_mean:.2f} Â± {sparse_std:.2f}\"\n",
    "            \n",
    "            # Store the results for this (mu, P) combination in a dictionary.\n",
    "            final_rows.append({\n",
    "                \"mu\": mu,\n",
    "                \"P\": P,\n",
    "                \"HMM (mean Â± std)\": f\"{hmm_mean:.2f} Â± {hmm_std:.2f}\",\n",
    "                \"Jump (mean Â± std)\": f\"{jump_mean:.2f} Â± {jump_std:.2f}\",\n",
    "                \"Sparse Jump (mean Â± std)\": sparse_str,\n",
    "                \"p-value (Jump vs Sparse)\": f\"{pval:.3g}\"\n",
    "            })\n",
    "            \n",
    "            # Print a message when this combination is finished\n",
    "            print(f\"Finished analysis for mu = {mu}, P = {P}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Convert the final results into a pandas DataFrame and display it.\n",
    "    df_results = pd.DataFrame(final_rows)\n",
    "    print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
