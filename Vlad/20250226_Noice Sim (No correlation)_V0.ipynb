{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Study for Noise Filtering\n",
    "\n",
    "This is the v0 for the simulation study on the sparse jump model comparison with HMM, to show that SJM is able to filter away noisy data by using the weighting in the algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from jumpmodels.sparse_jump import SparseJumpModel    # Sparse JM class\n",
    "from jumpmodels.jump import JumpModel   \n",
    "from scipy import stats \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Simulation & Utility Functions\n",
    "def simulate_data(T, P, mu, random_state=None): \"\"\" Simulate data from a 3-state Gaussian HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data(T, P, mu, random_state=None):\n",
    "    \"\"\"\n",
    "    Simulate data from a 2-state Gaussian HMM.\n",
    "    \n",
    "    Parameters:\n",
    "        T (int): Number of observations.\n",
    "        P (int): Total number of features (only first 15 are informative).\n",
    "        mu (float): Signal magnitude for informative features.\n",
    "        random_state (int or None): Seed for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "        X (ndarray): Simulated observations (T x P).\n",
    "        states (ndarray): True state sequence (length T).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    \n",
    "    # Transition matrix for 2 states\n",
    "    transmat = np.array([[0.9980, 0.0020],\n",
    "                         [0.0114, 0.9886]])\n",
    "    \n",
    "    # Compute stationary distribution\n",
    "    eigvals, eigvecs = np.linalg.eig(transmat.T)\n",
    "    stat = np.real(eigvecs[:, np.isclose(eigvals, 1)])\n",
    "    stat = stat[:, 0]\n",
    "    stat = stat / np.sum(stat)\n",
    "    \n",
    "    # Generate state sequence\n",
    "    states = np.zeros(T, dtype=int)\n",
    "    states[0] = rng.choice(np.arange(2), p=stat)\n",
    "    for t in range(1, T):\n",
    "        states[t] = rng.choice(np.arange(2), p=transmat[states[t-1]])\n",
    "    \n",
    "    # Define means for each state: state 0 = -mu, state 1 = mu for first 15 features.\n",
    "    means = np.zeros((2, P))\n",
    "    if P >= 15:\n",
    "        means[0, :15] = -mu\n",
    "        means[1, :15] = mu\n",
    "    else:\n",
    "        means[0, :P] = -mu\n",
    "        means[1, :P] = mu\n",
    "    \n",
    "    # Generate observations: N(means[state], I_P)\n",
    "    X = np.zeros((T, P))\n",
    "    for t in range(T):\n",
    "        X[t] = rng.normal(loc=means[states[t]], scale=1.0, size=P)\n",
    "    \n",
    "    return X, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Aligning Predicted Labels With True Labels using the Hungarian Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels(true_labels, pred_labels):\n",
    "    \"\"\"\n",
    "    Align predicted labels with true labels using the Hungarian algorithm.\n",
    "    \n",
    "    Returns:\n",
    "        aligned (ndarray): Predicted labels after optimal permutation.\n",
    "    \"\"\"\n",
    "    D = confusion_matrix(true_labels, pred_labels)\n",
    "    row_ind, col_ind = linear_sum_assignment(-D)\n",
    "    mapping = {col: row for row, col in zip(row_ind, col_ind)}\n",
    "    aligned = np.array([mapping[x] for x in pred_labels])\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up the function to calcuate the BAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bac(true_states, pred_states):\n",
    "    \"\"\"\n",
    "    Compute the Balanced Accuracy (BAC) after aligning the predicted state labels.\n",
    "    \"\"\"\n",
    "    aligned_pred = align_labels(true_states, pred_states)\n",
    "    return balanced_accuracy_score(true_states, aligned_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions for model formulation\n",
    "\n",
    "### 4.1 HMM With Nystrup (2021) initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hmm(X, n_components=2, random_state=None):\n",
    "    \"\"\"\n",
    "    Fit a Gaussian HMM to the data X.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Data matrix.\n",
    "        n_components (int): Number of hidden states.\n",
    "        random_state (int or None): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        pred_states (ndarray): Predicted state sequence using Viterbi decoding.\n",
    "    \"\"\"\n",
    "    model = hmm.GaussianHMM(\n",
    "        n_components=n_components,\n",
    "        covariance_type='diag',\n",
    "        n_iter=100,\n",
    "        random_state=random_state,\n",
    "        init_params=\"mc\",\n",
    "        covars_prior=1.0\n",
    "    )\n",
    "    model.startprob_ = np.full(n_components, 1.0 / n_components)\n",
    "    transmat = np.full((n_components, n_components), 0.05 / (n_components - 1))\n",
    "    np.fill_diagonal(transmat, 0.95)\n",
    "    model.transmat_ = transmat\n",
    "    \n",
    "    model.fit(X)\n",
    "    pred_states = model.predict(X)\n",
    "    return pred_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Normal (Standard) Jump Model with Grid Search over λ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_jump_model_grid_search(X, true_states, n_components=2, random_state=None):\n",
    "    \"\"\"\n",
    "    Perform a grid search over lambda values for the jump model.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Data matrix.\n",
    "        true_states (ndarray): True hidden state sequence.\n",
    "        n_components (int): Number of states.\n",
    "        random_state (int or None): Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        best_labels (ndarray): Predicted state sequence for the best lambda.\n",
    "        best_bac (float): Best balanced accuracy achieved.\n",
    "    \"\"\"\n",
    "    lambda_values = np.logspace(-2, 4, 14)\n",
    "    best_bac = -1\n",
    "    best_labels = None\n",
    "    \n",
    "    for lam in lambda_values:\n",
    "        model = JumpModel(\n",
    "            n_components=n_components,\n",
    "            jump_penalty=lam,\n",
    "            cont=False,\n",
    "            max_iter=10,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        model.fit(X)\n",
    "        labels = model.labels_\n",
    "        bac = calculate_bac(true_states, labels)\n",
    "        if bac > best_bac:\n",
    "            best_bac = bac\n",
    "            best_labels = labels\n",
    "    \n",
    "    return best_labels, best_bac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Sparse Jump Model with Grid Search over λ and kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sparse_jump_model_grid_search(X, true_states, n_components=2, random_state=None):\n",
    "    \"\"\"\n",
    "    Perform a grid search for the best combination of jump_penalty (lambda) and feature selection\n",
    "    level for the Sparse Jump Model (SJM).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        Data matrix of shape (T, P).\n",
    "    true_states : ndarray\n",
    "        True hidden state sequence.\n",
    "    n_components : int, default=2\n",
    "        Number of hidden states.\n",
    "    random_state : int or None, optional\n",
    "        Seed for reproducibility.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_labels : ndarray\n",
    "        Predicted state sequence for the best combination.\n",
    "    best_bac : float\n",
    "        The best balanced accuracy achieved.\n",
    "    \"\"\"\n",
    "    lambdas = np.logspace(-1, 2, 7)\n",
    "    p = X.shape[1]\n",
    "    kappas = np.linspace(1, np.sqrt(p), 14)\n",
    "    \n",
    "    best_bac = -1\n",
    "    best_labels = None\n",
    "    \n",
    "    for lam in lambdas:\n",
    "        for kappa in kappas:\n",
    "            max_feats = kappa**2\n",
    "            model = SparseJumpModel(\n",
    "                n_components=n_components,\n",
    "                jump_penalty=lam,\n",
    "                cont=False,\n",
    "                max_feats=max_feats,\n",
    "                max_iter=10,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            model.fit(X)\n",
    "            labels = model.labels_\n",
    "            bac = calculate_bac(true_states, labels)\n",
    "            if bac > best_bac:\n",
    "                best_bac = bac\n",
    "                best_labels = labels\n",
    "    \n",
    "    return best_labels, best_bac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Main Execution\n",
    " We split the code into three sections for each model and then combine results at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting simulations for mu = 0.1, P = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/Users/vlad/Desktop/git/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m jump_bac_list\u001b[38;5;241m.\u001b[39mappend(bac_jump)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Sparse Jump Model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m _, bac_sparse \u001b[38;5;241m=\u001b[39m run_sparse_jump_model_grid_search(X, true_states, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39msim)\n\u001b[1;32m     32\u001b[0m sparse_bac_list\u001b[38;5;241m.\u001b[39mappend(bac_sparse)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Completed simulation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_simulations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for mu = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmu\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, P = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mP\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m, in \u001b[0;36mrun_sparse_jump_model_grid_search\u001b[0;34m(X, true_states, n_components, random_state)\u001b[0m\n\u001b[1;32m     33\u001b[0m max_feats \u001b[38;5;241m=\u001b[39m kappa\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     34\u001b[0m model \u001b[38;5;241m=\u001b[39m SparseJumpModel(\n\u001b[1;32m     35\u001b[0m     n_components\u001b[38;5;241m=\u001b[39mn_components,\n\u001b[1;32m     36\u001b[0m     jump_penalty\u001b[38;5;241m=\u001b[39mlam,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mrandom_state\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m     43\u001b[0m labels \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[1;32m     44\u001b[0m bac \u001b[38;5;241m=\u001b[39m calculate_bac(true_states, labels)\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/jumpmodels/sparse_jump.py:380\u001b[0m, in \u001b[0;36mSparseJumpModel.fit\u001b[0;34m(self, X, ret_ser, sort_by)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m: jm\u001b[38;5;241m.\u001b[39mcenters_ \u001b[38;5;241m=\u001b[39m centers_unweighted \u001b[38;5;241m*\u001b[39m feat_weights    \n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# fit JM on weighted data\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m jm\u001b[38;5;241m.\u001b[39mfit(X, ret_ser\u001b[38;5;241m=\u001b[39mret_ser, feat_weights\u001b[38;5;241m=\u001b[39mfeat_weights, sort_by\u001b[38;5;241m=\u001b[39msort_by)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Step 2: optimize w\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# update (unweighted) centers\u001b[39;00m\n\u001b[1;32m    383\u001b[0m centers_unweighted \u001b[38;5;241m=\u001b[39m weighted_mean_cluster(X_arr, jm\u001b[38;5;241m.\u001b[39mproba_)\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/jumpmodels/jump.py:519\u001b[0m, in \u001b[0;36mJumpModel.fit\u001b[0;34m(self, X, ret_ser, feat_weights, sort_by)\u001b[0m\n\u001b[1;32m    517\u001b[0m     centers_ \u001b[38;5;241m=\u001b[39m weighted_mean_cluster(X_arr, proba_) \n\u001b[1;32m    518\u001b[0m     \u001b[38;5;66;03m# E step\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m     proba_, labels_, val_ \u001b[38;5;241m=\u001b[39m do_E_step(X_arr, centers_, jump_penalty_mx, prob_vecs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob_vecs)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_init_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-th init. val: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# compare with previous initializations\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/jumpmodels/jump.py:214\u001b[0m, in \u001b[0;36mdo_E_step\u001b[0;34m(X, centers_, penalty_mx, prob_vecs, return_value_mx)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_value_mx: \u001b[38;5;28;01mreturn\u001b[39;00m dp(loss_mx, penalty_mx, return_value_mx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# do a full E step\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m labels_, val_ \u001b[38;5;241m=\u001b[39m dp(loss_mx, penalty_mx, return_value_mx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)     \u001b[38;5;66;03m# output labels_ is of type int\u001b[39;00m\n\u001b[1;32m    215\u001b[0m proba_ \u001b[38;5;241m=\u001b[39m raise_JM_labels_to_proba(labels_, n_c, prob_vecs)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proba_, labels_, val_\n",
      "File \u001b[0;32m~/Desktop/git/anaconda3/lib/python3.12/site-packages/jumpmodels/jump.py:130\u001b[0m, in \u001b[0;36mdp\u001b[0;34m(loss_mx, penalty_mx, return_value_mx)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# DP iteration\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_s):\n\u001b[0;32m--> 130\u001b[0m     values[t] \u001b[38;5;241m=\u001b[39m loss_mx[t] \u001b[38;5;241m+\u001b[39m (values[t\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m+\u001b[39m penalty_mx)\u001b[38;5;241m.\u001b[39mmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# values[t-1][:, np.newaxis] turns the (t-1)-th row into a column\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_value_mx:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- Settings ---\n",
    "    T = 500\n",
    "    mu_values = [0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "    p_values = [15, 30, 60, 150, 300]\n",
    "    n_simulations = 10\n",
    "    \n",
    "    final_rows = []\n",
    "    \n",
    "    for mu in mu_values:\n",
    "        for P in p_values:\n",
    "            hmm_bac_list = []\n",
    "            jump_bac_list = []\n",
    "            sparse_bac_list = []\n",
    "            \n",
    "            print(f\"\\nStarting simulations for mu = {mu}, P = {P}\")\n",
    "            for sim in range(n_simulations):\n",
    "                X, true_states = simulate_data(T, P, mu, random_state=sim)\n",
    "                \n",
    "                # Gaussian HMM\n",
    "                pred_hmm = run_hmm(X, n_components=2, random_state=sim)\n",
    "                bac_hmm = calculate_bac(true_states, pred_hmm)\n",
    "                hmm_bac_list.append(bac_hmm)\n",
    "                \n",
    "                # Normal Jump Model\n",
    "                _, bac_jump = run_jump_model_grid_search(X, true_states, n_components=2, random_state=sim)\n",
    "                jump_bac_list.append(bac_jump)\n",
    "                \n",
    "                # Sparse Jump Model\n",
    "                _, bac_sparse = run_sparse_jump_model_grid_search(X, true_states, n_components=2, random_state=sim)\n",
    "                sparse_bac_list.append(bac_sparse)\n",
    "                \n",
    "                print(f\"  Completed simulation {sim+1}/{n_simulations} for mu = {mu}, P = {P}\", flush=True)\n",
    "            \n",
    "            hmm_mean, hmm_std = np.mean(hmm_bac_list), np.std(hmm_bac_list)\n",
    "            jump_mean, jump_std = np.mean(jump_bac_list), np.std(jump_bac_list)\n",
    "            sparse_mean, sparse_std = np.mean(sparse_bac_list), np.std(sparse_bac_list)\n",
    "            \n",
    "            tstat, pval = stats.ttest_rel(jump_bac_list, sparse_bac_list)\n",
    "            if (pval < 0.05) and (sparse_mean > jump_mean):\n",
    "                sparse_str = f\"**{sparse_mean:.2f} ± {sparse_std:.2f}**\"\n",
    "            else:\n",
    "                sparse_str = f\"{sparse_mean:.2f} ± {sparse_std:.2f}\"\n",
    "            \n",
    "            final_rows.append({\n",
    "                \"mu\": mu,\n",
    "                \"P\": P,\n",
    "                \"HMM (mean ± std)\": f\"{hmm_mean:.2f} ± {hmm_std:.2f}\",\n",
    "                \"Jump (mean ± std)\": f\"{jump_mean:.2f} ± {jump_std:.2f}\",\n",
    "                \"Sparse Jump (mean ± std)\": sparse_str,\n",
    "                \"p-value (Jump vs Sparse)\": f\"{pval:.3g}\"\n",
    "            })\n",
    "            \n",
    "            print(f\"Finished analysis for mu = {mu}, P = {P}: HMM BAC = {hmm_mean:.3f}, Jump BAC = {jump_mean:.3f}, Sparse Jump BAC = {sparse_mean:.3f}\", flush=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df_results = pd.DataFrame(final_rows)\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(df_results, flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
